# -*- coding: utf-8 -*-
"""A2_Part3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/mavho/CSE143/blob/master/A2_Part3.ipynb

# Setup

Installs the correct packages.

First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0.
"""

# Commented out IPython magic to ensure Python compatibility.
# Python ≥3.5 is required
import sys
assert sys.version_info >= (3, 5)

# Scikit-Learn ≥0.20 is required
import sklearn
assert sklearn.__version__ >= "0.20"

try:
    # %tensorflow_version only exists in Colab.
#     %tensorflow_version 2.x
    #pip install -q -U tensorflow-addons
    IS_COLAB = True
except Exception:
    IS_COLAB = False

# TensorFlow ≥2.0 is required
import tensorflow as tf
from tensorflow import keras
assert tf.__version__ >= "2.0"

if not tf.test.is_gpu_available():
    print("No GPU was detected. LSTMs and CNNs can be very slow without a GPU.")
    if IS_COLAB:
        print("Go to Runtime > Change runtime and select a GPU hardware accelerator.")

# Common imports
import numpy as np
import os

# to make this notebook's output stable across runs
np.random.seed(42)
tf.random.set_seed(42)

# To plot pretty figures
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "nlp"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)
os.makedirs(IMAGES_PATH, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)

"""# Sentiment Analysis

Below is an example of how to do custom preprocessing
"""

import tensorflow_datasets as tfds
train_data, dev_data, test_data = tfds.load("imdb_reviews", split=('train[:60%]', 'train[60%:]', 'test'), as_supervised=True)

for X_batch, y_batch in train_data.batch(2).take(1):
    for review, label in zip(X_batch.numpy(), y_batch.numpy()):
        print("Review:", review.decode("utf-8")[:200], "...")
        print("Label:", label, "= Positive" if label else "= Negative")
        print()

def preprocess(X_batch, y_batch):
    X_batch = tf.strings.substr(X_batch, 0, 300)
    X_batch = tf.strings.regex_replace(X_batch, rb"<br\s*/?>", b" ")
    X_batch = tf.strings.regex_replace(X_batch, b"[^a-zA-Z']", b" ")
    X_batch = tf.strings.split(X_batch)
    return X_batch.to_tensor(default_value=b"<pad>"), y_batch

"""Construct the vocaulary, counts occurence of each word."""

from collections import Counter

vocabulary = Counter()
for X_batch, y_batch in train_data.batch(32).map(preprocess):
    for review in X_batch:
        vocabulary.update(list(review.numpy()))

"""Look at only the first 10000 most common words in the vocab"""

vocab_size = 10000
truncated_vocabulary = [
    word for word, count in vocabulary.most_common()[:vocab_size]]

"""# Using GloVe Embeddings

Get and extract Glove datasets
"""

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove.6B.zip
!rm glove.6B.zip
!mkdir glove
!mv *.txt glove

word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}
for word in b"This This this".split():
    print(word_to_id.get(word) or vocab_size)

words = tf.constant(truncated_vocabulary)
word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)
vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)
num_oov_buckets = 1000
table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)

train_set = train_data.batch(32).map(preprocess)
for X_batch, y_batch in train_set.batch(2).take(1):
    for review, label in zip(X_batch.numpy(), y_batch.numpy()):
        print("Review:", review[:200], "...")
        print("Label:", label)
        print()

def encode_words(X_batch, y_batch):
    return table.lookup(X_batch), y_batch
    
train_set = train_set.map(encode_words).prefetch(1)

for X_batch, y_batch in train_set.batch(1).take(1):
    for review, label in zip(X_batch.numpy(), y_batch.numpy()):
        print("Review:", review[:200], "...")
        print("Label:", label)
        print()

embeddings_index = dict()
f = open('/content/glove/glove.6B.100d.txt')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
print('Loaded %s word vectors.' % len(embeddings_index))

embed_size = vocab_size+num_oov_buckets
# create a weight matrix for words in training docs
embedding_matrix = np.zeros((embed_size, 100))
for word, index in word_to_id.items():
    if index > embed_size - 1:
        break
    else:
        embedding_vector = embeddings_index.get(word.decode('utf8'))
        if embedding_vector is not None:
            embedding_matrix[index] = embedding_vector

print(embedding_matrix[word_to_id[b'the']])

glove_model = keras.Sequential([
    keras.layers.Embedding(embed_size,output_dim=100,weights=[embedding_matrix],input_length=100, trainable=False),
    keras.layers.GRU(200, activation="relu",dropout=0.15),
    keras.layers.Flatten(),
    keras.layers.Dense(1, activation="sigmoid")
])
glove_model.compile(loss="binary_crossentropy", optimizer="adam",
              metrics=["accuracy"])

batch_size = 32
history = glove_model.fit(train_set, epochs=10)

glove_model.summary()

test_set = test_data.batch(512).map(preprocess)
test_set = test_set.map(encode_words).prefetch(1)
dev_set = dev_data.batch(32).map(preprocess)
dev_set = dev_set.map(encode_words).prefetch(1)

loss,accurracy = glove_model.evaluate(dev_set, steps=10, verbose=True)

loss,accuracy = glove_model.evaluate(test_set, steps=10,verbose=True)

from numpy import dot
from numpy.linalg import norm

negative_sentence = b"I absolutely hated this movie"
positive_sentence = b"I absolutely loved this movie"

pos_arr = []
neg_arr = []

count = 0
for i in positive_sentence.split():
  count += 1
  pos_arr.append(word_to_id.get(i) or vocab_size )
for i in range(100 - count):
  pos_arr.append(word_to_id.get(b"<pad>"))
count = 0
for i in negative_sentence.split():
  count += 1
  neg_arr.append(word_to_id.get(i) or vocab_size )
for i in range(100 - count):
  neg_arr.append(word_to_id.get(b"<pad>"))

pos_arr = np.array(pos_arr)
pos_arr = pos_arr.reshape(100,1).T

neg_arr = np.array(neg_arr)
neg_arr = neg_arr.reshape(100,1).T

print("The glove embedding for '{}' is {} ".format('hated', embedding_matrix[word_to_id[b'hated']]))
print("The glove embedding for '{}' is {} ".format('loved', embedding_matrix[word_to_id[b'loved']]))

"""Cosine embedding check and prediction results"""

print(positive_sentence)
print("Positive results: {}".format(glove_model.predict(pos_arr)))

print(negative_sentence)
print("Negative results: {}".format(glove_model.predict(neg_arr)))

cos_sim = dot(pos_arr.flatten(),neg_arr.flatten())/(norm(pos_arr.flatten())*norm(neg_arr.flatten()))
print(cos_sim)

print(glove_model.predict_classes(neg_arr))
print(glove_model.predict_classes(pos_arr))