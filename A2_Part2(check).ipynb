{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of A2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hfvGEfzDHdWk"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mavho/CSE143/blob/master/A2_Part2(check).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mYpcy4ILYOl",
        "colab_type": "text"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Installs the correct packages.\n",
        "\n",
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z__lfNuUXsps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    #pip install -q -U tensorflow-addons\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.test.is_gpu_available():\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"nlp\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlVZr_-iL2yI",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVO6Ik3aMIYL",
        "colab_type": "text"
      },
      "source": [
        "Loads the data into a train and test set.\n",
        "X_train is a list of reviews, represented as numpy arrays of integers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rj0XCzPXsp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_test), (X_valid, y_test) = keras.datasets.imdb.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgcIK6qVEV3k",
        "colab_type": "text"
      },
      "source": [
        "row 0, up to 10th column\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svNwJbW1DZK5",
        "colab_type": "code",
        "outputId": "0ce1f545-3b05-4b5a-beed-700066ba853e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "X_train[0][:10]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCunAKbaDZLA",
        "colab_type": "code",
        "outputId": "03e24a0d-81c6-4ac8-eb91-0e9ad940b2d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "word_index = keras.datasets.imdb.get_word_index()\n",
        "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n",
        "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "    id_to_word[id_] = token\n",
        "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<sos> this film was just brilliant casting location scenery story'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj32n_PKEnNr",
        "colab_type": "text"
      },
      "source": [
        "Below is an example of how to do custom preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSbrtR0nDZLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "train_data, dev_data, test_data = tfds.load(\"imdb_reviews\", split=('train[:60%]', 'train[60%:]', 'test'), as_supervised=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NijMHL1zDZLX",
        "colab_type": "code",
        "outputId": "beb74c93-6c2c-494e-d8c6-6c5e37705798",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        }
      },
      "source": [
        "for X_batch, y_batch in train_data.batch(2).take(1):\n",
        "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
        "        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
        "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
        "        print()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
            "Label: 0 = Negative\n",
            "\n",
            "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
            "Label: 0 = Negative\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1lzP_KmDZLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(X_batch, y_batch):\n",
        "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
        "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
        "    X_batch = tf.strings.split(X_batch)\n",
        "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCc_KRceDZLf",
        "colab_type": "code",
        "outputId": "195a213d-929e-4421-988b-77ae6d440553",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "preprocess(X_batch, y_batch)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(2, 53), dtype=string, numpy=\n",
              " array([[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie',\n",
              "         b\"Don't\", b'be', b'lured', b'in', b'by', b'Christopher',\n",
              "         b'Walken', b'or', b'Michael', b'Ironside', b'Both', b'are',\n",
              "         b'great', b'actors', b'but', b'this', b'must', b'simply', b'be',\n",
              "         b'their', b'worst', b'role', b'in', b'history', b'Even',\n",
              "         b'their', b'great', b'acting', b'could', b'not', b'redeem',\n",
              "         b'this', b\"movie's\", b'ridiculous', b'storyline', b'This',\n",
              "         b'movie', b'is', b'an', b'early', b'nineties', b'US',\n",
              "         b'propaganda', b'pi', b'<pad>', b'<pad>', b'<pad>'],\n",
              "        [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep',\n",
              "         b'during', b'films', b'but', b'this', b'is', b'usually', b'due',\n",
              "         b'to', b'a', b'combination', b'of', b'things', b'including',\n",
              "         b'really', b'tired', b'being', b'warm', b'and', b'comfortable',\n",
              "         b'on', b'the', b'sette', b'and', b'having', b'just', b'eaten',\n",
              "         b'a', b'lot', b'However', b'on', b'this', b'occasion', b'I',\n",
              "         b'fell', b'asleep', b'because', b'the', b'film', b'was',\n",
              "         b'rubbish', b'The', b'plot', b'development', b'was', b'constant',\n",
              "         b'Cons']], dtype=object)>,\n",
              " <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0])>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAoo8pM5FNE7",
        "colab_type": "text"
      },
      "source": [
        "Construct the vocaulary, counts occurence of each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAH0e2-7DZLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "vocabulary = Counter()\n",
        "for X_batch, y_batch in train_data.batch(16).map(preprocess):\n",
        "    for review in X_batch:\n",
        "        vocabulary.update(list(review.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybqSIIBIFffo",
        "colab_type": "text"
      },
      "source": [
        "Look at only the first 10000 most common words in the vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy2djDvwDZLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 10000\n",
        "truncated_vocabulary = [\n",
        "    word for word, count in vocabulary.most_common()[:vocab_size]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn_abEQ1FoPN",
        "colab_type": "text"
      },
      "source": [
        "An example of what and how the words are indexed.\n",
        "This, movie and was was found in the vocab, so the scores are lower, however faaaaantastic wasn't found, so it has a high score?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "717eTCiZF3HM",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeNN68MzDZL2",
        "colab_type": "code",
        "outputId": "cb5f5a90-0cb0-419a-9a06-8c6bc40001d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
        "for word in b\"This movie was faaaaaantastic\".split():\n",
        "    print(word_to_id.get(word) or vocab_size)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22\n",
            "12\n",
            "11\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DemoBWdbF7m0",
        "colab_type": "text"
      },
      "source": [
        "replace each word with it's ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg5Zb6Z-DZL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = tf.constant(truncated_vocabulary)\n",
        "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
        "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "num_oov_buckets = 1000\n",
        "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WetvRUzYDZL9",
        "colab_type": "code",
        "outputId": "4f770f3b-1bb2-4524-cccb-7ff18242ce0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irsbt92AGPkX",
        "colab_type": "text"
      },
      "source": [
        "Creating the final training set.\n",
        "We batch the reviews, and using the preprocess function, encode them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk8TOT8uDZMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_words(X_batch, y_batch):\n",
        "    return table.lookup(X_batch), y_batch\n",
        "\n",
        "train_set = train_data.batch(16).map(preprocess)\n",
        "train_set = train_set.map(encode_words).prefetch(1)\n",
        "dev_set = dev_data.batch(16).map(preprocess)\n",
        "dev_set = dev_set.map(encode_words).prefetch(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBG5KqZoGoXz",
        "colab_type": "text"
      },
      "source": [
        "What the train set looks like..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0lBicRHDZMF",
        "colab_type": "code",
        "outputId": "66827027-3f64-4726-b067-ceb9cd0d5742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for X_batch, y_batch in train_set.take(1):\n",
        "    print(X_batch)\n",
        "    print(y_batch)\n",
        "for X_batch, y_batch in dev_set.take(1):\n",
        "    print(X_batch)\n",
        "    print(y_batch)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[   22    11    28   338   287    12   635    25  7697     8    31  1242\n",
            "   3589    45   490 10966  1806    26    67   156    20     9   196   365\n",
            "     25    91   145   280     8   447   486    91    67    86    99    24\n",
            "   6287     9  1609   622   569    22    12     7    28   356  4401  1192\n",
            "   2053 10325     0     0     0     0     0     0     0     0     0]\n",
            " [    6    21    71   523     5   881  1719   370    79    20     9     7\n",
            "    513   690     5     2  1555     3   228   826    51  1193   127  2054\n",
            "      4  5007    19     1 10881     4   327    41  6288     2   161   316\n",
            "     19     9  4402     6  1169  1719    81     1    15    11  1556    14\n",
            "     85  1170    11  3151  8703     0     0     0     0     0     0]\n",
            " [ 3278  6289     1 10808  6290 10367     8     2   790  2101     4  2269\n",
            "   1095     4  1610  6931   244   750   418    18    52   205   395     5\n",
            "    102   112   227    19   335     2 10255  1171     1    80     3  5768\n",
            "   1213 10610     5 10542   882     2 10244   849     2 10244     4     5\n",
            "    696     0     0     0     0     0     0     0     0     0     0]\n",
            " [   22     7     1   243     3    15    17     2 10613  1964  2390    57\n",
            "      1   597     3     1   234    72   162  1846    16    98   283  1115\n",
            "     18    27 10424    92     2   202  3766  2993     4 10998    17     2\n",
            "    441     3   654  3152   418    36  5008     4  3944  1847    18   205\n",
            "  10499  3425     1    85   451   136    26    70 10520     5  5388]\n",
            " [  199   528    21  1428    32     1   426    13   162  3945     8     9\n",
            "     15    26   746   338  1509    14    85    43 10127   288     1 10891\n",
            "      3     1   771 10828   193   504    26   214    52   230     5    25\n",
            "   6932    20    57    70   504    26   214    52   543     1 10229     3\n",
            "      2   220   140   529   153     1   529   661     0     0     0]\n",
            " [   22     7     2    15    76   152    25    60    31  2604   946     8\n",
            "  10057    31    45  3590    36    28  2796  6291    38     7    28  3279\n",
            "   1383     4  2693  1230     3 10767     8     2  1409   290    98  3280\n",
            "      4    98 10426    14   290     7   294    31    23     3     1    83\n",
            "    452   187  1410   850     8   845     0     0     0     0     0]\n",
            " [  988    27    21 10895  4403    18  2797 10560 10414   427   427  1965\n",
            "  10472     3  1611  2270   781   134     8     1    55   213   323   237\n",
            "   1720     9   108    39    70 10692   623 10198    18 10249  6933 10417\n",
            "     36    14   312   662     4   191     1 10307  2271     3     1 10804\n",
            "      0     0     0     0     0     0     0     0     0     0     0]\n",
            " [   14    15     7   295    19     2  2605   135   433 10656  5389 10051\n",
            "    735     2   259     3   310  1055  2010  6934 10854  1213     3 10723\n",
            "      4   537  1298     4  4676     6    21   224    32   310    14    55\n",
            "    105    26   282    14   208   563    28  2994    13   126    24   227\n",
            "    134   112 10942     0     0     0     0     0     0     0     0]\n",
            " [    6    51   110     1  1358   215     4   896   667    79     3     1\n",
            "   3426     4    98    81     3     1 10497    13  1320     8   130   238\n",
            "    194     1 10335   426     5    25     8   200    79     4    10   151\n",
            "    682    84    52    99   587  2995  4677    14   292     6    11   481\n",
            "     31     9    15    11    81    10   233 10564     0     0     0]\n",
            " [ 1151     9    23   251    51     2  2798  1096   163    10  2606   157\n",
            "      2  3767 10475     7     1    55   438     3     2   178   860   976\n",
            "   3591    37     7   430   284    45  1578    17    33   243     3    86\n",
            "      4  1557   668    54     9    12     7    30    38     7   295    19\n",
            "      1  6292 10475 10401   735     2   169  2799     0     0     0]\n",
            " [ 2221     2 10845   297     6    11 10846   144     1  2272   194     9\n",
            "   4160     3     2    12   172    68    52   459   133     9     7   122\n",
            "    117 10851 10537    17  8704 10872 10619    70   657  1021   905  1529\n",
            "    153   189   126   570  4403   231     9   280    97   977   655     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0]\n",
            " [ 8705    15    30   310  4678  2319    36 10025   453    60   886    30\n",
            "      8  8706  5009    37   230     5    88    91   947   149   334   232\n",
            "   1684     7   146  3592     1 10334    17  1721    34    52 10375   134\n",
            "      5   408   636     5   658     1  1721    36 10566     2  2320  2102\n",
            "   7698    50  2154 10029  5010     0     0     0     0     0     0]\n",
            " [   22   269     3     1  5390   273    73   798  3768   385   378    31\n",
            "   6935  8707   329   557   967   926    18  7699  3769  1011     9    48\n",
            "    214     1  2607    17     1  1384    11     5   231   157  1059   861\n",
            "      4   107    10   395  1385     4   211   197     4  3153     5   109\n",
            "     16     9    15  8707     0     0     0     0     0     0     0]\n",
            " [ 5769     1  2391    19  4404  1243  3281  4161     4 10483     1 10344\n",
            "     17     9   202   286 10732     2 10575  2524 10960     3  2392    16\n",
            "      2 10530 10321 10505    29 10002 10971    37   261   765   230    33\n",
            "    438    19     9   167  1097     4   203    82    17     1   597     3\n",
            "    270   759   948     0     0     0     0     0     0     0     0]\n",
            " [ 6936 10913  1579   452    15 10198    53     3 10591 10185 10864    45\n",
            "  10802  5770  5771   906     3   691   124 10377    30     4   274    17\n",
            "     54     7    10     6   839   236     1   247   106   144    32     1\n",
            "    216  3282     2 10436  1012    19     1 10282     4  2996     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0]\n",
            " [   38    11  2155     4  1285   172     2   448     3     2   179     6\n",
            "   3946     1   420   419   839 10054     8   225    22     7    24     2\n",
            "   3593  1342    24    77   887    10  8708  1343    11    41   118   367\n",
            "      5    25  1046  3283    11   487   524   487     6   406  1037    17\n",
            "    165     1   308     8   425    80   692     0     0     0     0]], shape=(16, 59), dtype=int64)\n",
            "tf.Tensor([0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0], shape=(16,), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[   22    11     2   790   353    23     3     1    93     3   277  1919\n",
            "   4997   237   154    17     2   888    16     2    47    13     7   106\n",
            "    106   872     1   838 10087   562    84    78     7   157     2   167\n",
            "     73  2112   371     7   236  3031     5   527     8     2  2068   646\n",
            "   4236     8    76    50    11  1978   547 10736  1179 10821   151   674]\n",
            " [ 2963  1409  6707  1537     5 10322 10856    17  4521  1205   628    52\n",
            "     88     2  1196  6493     8 10210  5586     4    23     3     1   701\n",
            "   6504     2  1584   632  1211     8     1  2039    14   592  2598     7\n",
            "    683     1   876     4     1  2991   929 10404 10192     7     2  2045\n",
            "    278   286   996  1819     0     0     0     0     0     0     0     0]\n",
            " [  570 10374  2486     2   280    13    50    11     5 10723   716     8\n",
            "  10369     1  3445   666   153    10     7     2   280    50   289    89\n",
            "     22   429   450   134    29     2    40  3624     4  5353    27   144\n",
            "      5     1   192  4314 10831   289     2   518  1668     4  8655  9955\n",
            "  10583    14    47   499     0     0     0     0     0     0     0     0]\n",
            " [   22   851    24   127     1   171    10  1348   122    18     2   325\n",
            "      8  3861  2478    39 10635     1   416 10196    13  1521  8996   283\n",
            "    665    16   744   823   119    14   852  2345 10890     1  1566   108\n",
            "    399 10320  1851    77     1   282  1488  8365     8  1187     2   441\n",
            "      3   111   298     2     0     0     0     0     0     0     0     0]\n",
            " [    6   517    96     6  3329     4    77   218   121    24     2   566\n",
            "    158   590     3 10370     6    95    13    10    11    41     2   444\n",
            "      4  1105    12     5   100   317   101    56     8    42   602   796\n",
            "   5201   268   113    13   578    57   482   115     2    12     4    27\n",
            "     41   248  9742    81    65   827   160    36     1    47     4     0]\n",
            " [    6   100   566   857    16    46  9147    17   566   857   764   588\n",
            "    140   728   135    76   107     2    12  2378  8747     8     2 10549\n",
            "   2079    78     7     2   290     4    78     7     2   322   277   881\n",
            "      8   110   128    21  5181     4   128  1600   129     1  5181     5\n",
            "   4255    45  1455   112     9   694     0     0     0     0     0     0]\n",
            " [10740     1  2082   269     3 10079 10442 10166   674 10498     7     2\n",
            "   1671     5   707   144    77    29     2  8143   225   116    66     3\n",
            "      9  7887   304     4  1659 10807   154  7829     4     6   226    21\n",
            "  10451    92     2  8095  5156  4804   393   197  1712     2   381   260\n",
            "    361     3   135     0     0     0     0     0     0     0     0     0]\n",
            " [  368 10016  2295   368 10032     7     1  1753    15    36 10711  1407\n",
            "    368 10349  1384  7366  5808 10331    14    12    76  4536    41    23\n",
            "    553   127     8    98   179   187   574 10798   369  1279 10649   705\n",
            "      1    47     3     1   331  1027   218  1933  1046  1074   262     2\n",
            "    542     4     2  2095     0     0     0     0     0     0     0     0]\n",
            " [ 1293   760   749  3899    31   760   476     8    76     2   187   229\n",
            "  10871     5  3986 10352   182    44    30     1  7479  6919   585     5\n",
            "     91   415    14   474  9086 10191   128   972    44    30     1 10757\n",
            "   6947  2278   951     1  7479  6919    13    39  2878     3 10601    61\n",
            "      3   377     0     0     0     0     0     0     0     0     0     0]\n",
            " [  275     6   196   109    13     9    11    23   756     3     2   240\n",
            "     12   968     1   223    13     1  4745    11   180   866     4    78\n",
            "     68    46  1301   536   142     1    15   469     5   614   634  3078\n",
            "     17    70  2526   292     6    11    24   481    14    47    11   176\n",
            "    180   186     1   208  1979     3     1 10865     0     0     0     0]\n",
            " [    6    96     9    12    29  3703     4    11  3908    29    90    58\n",
            "     10    11   443   295    19     1 10262  2875     6    11  1206     5\n",
            "     59    10  7974     2  7424     3   105   291   363  1421   272     2\n",
            "    435     3   291   363   122    10  3511    58    86    58   753   403\n",
            "    796     3  1836     0     0     0     0     0     0     0     0     0]\n",
            " [ 5734    92     9   384    41    18    10    11   398    19    28  2390\n",
            "    142     6    11   415    16     2  1518     3 10778  2057   437    21\n",
            "    927    10   379   261    64    21    71    93     6  1929     1  1824\n",
            "      3  4218  6813     4  3465  1709   277    43    40  3017     4   131\n",
            "      9   281    25   366     2  1279     0     0     0     0     0     0]\n",
            " [    6    41  1113     8     1   546   884    18   756    63   397     5\n",
            "    995  2169   144     1    12    14    85     7   584 10101   680  4236\n",
            "  10367   317   238    21     5   915     2  5598     8   797     5    88\n",
            "     92     1  3482 10777    14   167    13 10519    53   134     1    83\n",
            "     11     1    34   392  2722   580   238    68  8255     0     0     0]\n",
            " [ 1342    24    59     9    12    84    27  1231   159   373   410     1\n",
            "    192     3   346 10042   784    53     4    42   344 10032    13   114\n",
            "    374   510     3   346  4900   370    98   377    14    61   525   593\n",
            "     16    98   272    13    11    24 10958  2840    11     1   259  7510\n",
            "    710    56   995     2    58  1144     0     0     0     0     0     0]\n",
            " [ 6515 10430   393  4209    13    57   228    26   677   118  1124    23\n",
            "      7     2   566   193   228    26   108 10710  5042    52    26   801\n",
            "    153    57   160   265     1  3651  2901   953  1854    19   628    17\n",
            "   6145  1218 10584    50   828    92     1   343 10319    22   164    39\n",
            "  10458     0     0     0     0     0     0     0     0     0     0     0]\n",
            " [  221   786     4     6    68  1872    31     1  9045     2   648 10711\n",
            "   3104  1567    16     2  9359   992   122 10427  4262    17     1 10689\n",
            "     14    55   698    11   196    59   181    17   270   628     1   522\n",
            "      3    74   992   122     8     1   343   698   875    34  2597     4\n",
            "  10695     3     1   648   147     0     0     0     0     0     0     0]], shape=(16, 60), dtype=int64)\n",
            "tf.Tensor([1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 0], shape=(16,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlMkg8ujGyda",
        "colab_type": "text"
      },
      "source": [
        "Creates the model and trains it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGHgjn0dDZMI",
        "colab_type": "code",
        "outputId": "5a39eee4-4d35-49f1-fc3e-4b581c41693a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "source": [
        "embed_size = 128\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
        "                           mask_zero=True, # not shown in the book\n",
        "                           input_shape=[None]),\n",
        "    keras.layers.GRU(128,activation ='relu', dropout = .15),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_set, validation_data=dev_set, epochs=10)"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "938/938 [==============================] - 94s 100ms/step - loss: 0.5905 - accuracy: 0.6564 - val_loss: 0.5160 - val_accuracy: 0.7578\n",
            "Epoch 2/10\n",
            "938/938 [==============================] - 92s 98ms/step - loss: 0.3638 - accuracy: 0.8415 - val_loss: 0.6239 - val_accuracy: 0.7612\n",
            "Epoch 3/10\n",
            "938/938 [==============================] - 94s 100ms/step - loss: 0.2339 - accuracy: 0.9098 - val_loss: 0.6773 - val_accuracy: 0.7647\n",
            "Epoch 4/10\n",
            "938/938 [==============================] - 98s 105ms/step - loss: 0.1605 - accuracy: 0.9397 - val_loss: 0.7205 - val_accuracy: 0.7516\n",
            "Epoch 5/10\n",
            "938/938 [==============================] - 99s 106ms/step - loss: 0.1123 - accuracy: 0.9589 - val_loss: 0.7672 - val_accuracy: 0.7482\n",
            "Epoch 6/10\n",
            "938/938 [==============================] - 95s 102ms/step - loss: 0.0747 - accuracy: 0.9733 - val_loss: 1.4886 - val_accuracy: 0.7286\n",
            "Epoch 7/10\n",
            "938/938 [==============================] - 94s 100ms/step - loss: 0.0429 - accuracy: 0.9855 - val_loss: 1.6550 - val_accuracy: 0.7427\n",
            "Epoch 8/10\n",
            "938/938 [==============================] - 95s 101ms/step - loss: 0.0345 - accuracy: 0.9898 - val_loss: 1.4191 - val_accuracy: 0.7494\n",
            "Epoch 9/10\n",
            "938/938 [==============================] - 93s 100ms/step - loss: 0.0170 - accuracy: 0.9950 - val_loss: 1.6833 - val_accuracy: 0.7508\n",
            "Epoch 10/10\n",
            "938/938 [==============================] - 94s 101ms/step - loss: 0.0099 - accuracy: 0.9971 - val_loss: 2.1316 - val_accuracy: 0.7497\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWOkY5rU5-_t",
        "colab_type": "code",
        "outputId": "1dad686f-c42d-408e-a736-d1d87cc382c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "history.model.summary()\n"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, None, 128)         1408000   \n",
            "_________________________________________________________________\n",
            "gru_11 (GRU)                 (None, 128)               99072     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 1,507,201\n",
            "Trainable params: 1,507,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU9hOPzN1bz9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set = test_data.batch(512).map(preprocess)\n",
        "test_set = test_set.map(encode_words).prefetch(1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGkwuc1g-w7z",
        "colab_type": "text"
      },
      "source": [
        "embedds the test data, this is how well it performs?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcpN50qy9za5",
        "colab_type": "code",
        "outputId": "87eebd66-a5ed-49d3-be33-6fd1a97278ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "loss,accuracy = model.evaluate(test_set,steps=10)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 1s 83ms/step - loss: 2.3921 - accuracy: 0.7258\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}