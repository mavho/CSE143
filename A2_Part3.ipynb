{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2_Part3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mavho/CSE143/blob/master/A2_Part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mYpcy4ILYOl",
        "colab_type": "text"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Installs the correct packages.\n",
        "\n",
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z__lfNuUXsps",
        "colab_type": "code",
        "outputId": "f350c6b4-870d-4d41-cbf1-93d233b3c258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    #pip install -q -U tensorflow-addons\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.test.is_gpu_available():\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"nlp\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "WARNING:tensorflow:From <ipython-input-2-d2f2dcc24676>:21: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlVZr_-iL2yI",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj32n_PKEnNr",
        "colab_type": "text"
      },
      "source": [
        "Below is an example of how to do custom preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSbrtR0nDZLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "train_data, dev_data, test_data = tfds.load(\"imdb_reviews\", split=('train[:60%]', 'train[60%:]', 'test'), as_supervised=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NijMHL1zDZLX",
        "colab_type": "code",
        "outputId": "42bbfb4a-ce1f-4526-8cf7-4f5392e92253",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "for X_batch, y_batch in train_data.batch(2).take(1):\n",
        "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
        "        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
        "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
        "        print()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
            "Label: 0 = Negative\n",
            "\n",
            "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
            "Label: 0 = Negative\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1lzP_KmDZLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(X_batch, y_batch):\n",
        "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
        "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
        "    X_batch = tf.strings.split(X_batch)\n",
        "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAoo8pM5FNE7",
        "colab_type": "text"
      },
      "source": [
        "Construct the vocaulary, counts occurence of each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAH0e2-7DZLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "vocabulary = Counter()\n",
        "for X_batch, y_batch in train_data.batch(32).map(preprocess):\n",
        "    for review in X_batch:\n",
        "        vocabulary.update(list(review.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybqSIIBIFffo",
        "colab_type": "text"
      },
      "source": [
        "Look at only the first 10000 most common words in the vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy2djDvwDZLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 10000\n",
        "truncated_vocabulary = [\n",
        "    word for word, count in vocabulary.most_common()[:vocab_size]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfvGEfzDHdWk",
        "colab_type": "text"
      },
      "source": [
        "# Using GloVe Embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB342qdWoDAU",
        "colab_type": "text"
      },
      "source": [
        "Get and extract Glove datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RydXKVKvnpj",
        "colab_type": "code",
        "outputId": "fb11f727-84e6-44c2-a1eb-cf00479b4a8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "!rm glove.6B.zip\n",
        "!mkdir glove\n",
        "!mv *.txt glove"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-18 00:03:03--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-02-18 00:03:04--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-02-18 00:03:04--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.12MB/s    in 6m 30s  \n",
            "\n",
            "2020-02-18 00:09:34 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbEIy13HDFIV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "60d79fd1-1f79-4e23-c458-325e43e908ce"
      },
      "source": [
        "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
        "for word in b\"This This this\".split():\n",
        "    print(word_to_id.get(word) or vocab_size)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22\n",
            "22\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HyR9ENd2iHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = tf.constant(truncated_vocabulary)\n",
        "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
        "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "num_oov_buckets = 1000\n",
        "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsjqMEwU15H4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "5f19aa2d-a65e-4ddf-951d-4e8ae8e93ff2"
      },
      "source": [
        "train_set = train_data.batch(32).map(preprocess)\n",
        "for X_batch, y_batch in train_set.batch(2).take(1):\n",
        "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
        "        print(\"Review:\", review[:200], \"...\")\n",
        "        print(\"Label:\", label)\n",
        "        print()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: [[b'This' b'was' b'an' ... b'<pad>' b'<pad>' b'<pad>']\n",
            " [b'I' b'have' b'been' ... b'<pad>' b'<pad>' b'<pad>']\n",
            " [b'Mann' b'photographs' b'the' ... b'<pad>' b'<pad>' b'<pad>']\n",
            " ...\n",
            " [b'This' b'movie' b'never' ... b'went' b'st' b'<pad>']\n",
            " [b'Mike' b'Brady' b'Michael' ... b'<pad>' b'<pad>' b'<pad>']\n",
            " [b'Honestly' b'Barbra' b'I' ... b'<pad>' b'<pad>' b'<pad>']] ...\n",
            "Label: [0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0]\n",
            "\n",
            "Review: [[b'The' b'Confidential' b'part' ... b'<pad>' b'<pad>' b'<pad>']\n",
            " [b'I' b'was' b'thirteen' ... b'an' b'<pad>' b'<pad>']\n",
            " [b\"I'm\" b'sorry' b'I' ... b'<pad>' b'<pad>' b'<pad>']\n",
            " ...\n",
            " [b'A' b'hilarious' b'and' ... b'<pad>' b'<pad>' b'<pad>']\n",
            " [b'Judging' b'from' b'this' ... b'<pad>' b'<pad>' b'<pad>']\n",
            " [b'I' b\"didn't\" b'expect' ... b'<pad>' b'<pad>' b'<pad>']] ...\n",
            "Label: [0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6F7EBejCzsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_words(X_batch, y_batch):\n",
        "    return table.lookup(X_batch), y_batch\n",
        "    \n",
        "train_set = train_set.map(encode_words).prefetch(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-gv8iEg-CGmS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "450a898b-d38f-4c7c-fb33-2f320d8abc50"
      },
      "source": [
        "for X_batch, y_batch in train_set.batch(1).take(1):\n",
        "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
        "        print(\"Review:\", review[:200], \"...\")\n",
        "        print(\"Label:\", label)\n",
        "        print()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: [[  22   11   28 ...    0    0    0]\n",
            " [   6   21   71 ...    0    0    0]\n",
            " [3278 6289    1 ...    0    0    0]\n",
            " ...\n",
            " [  22   12  120 ...  332 1030    0]\n",
            " [1810 3594  490 ...    0    0    0]\n",
            " [2997 5393    6 ...    0    0    0]] ...\n",
            "Label: [0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjkIyMiHm7cf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "00c282f0-d852-446b-e67f-4f5557dc2db5"
      },
      "source": [
        "embeddings_index = dict()\n",
        "f = open('/content/glove/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icPw5JSAnBM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_size = vocab_size+num_oov_buckets\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((embed_size, 100))\n",
        "for word, index in word_to_id.items():\n",
        "    if index > embed_size - 1:\n",
        "        break\n",
        "    else:\n",
        "        embedding_vector = embeddings_index.get(word.decode('utf8'))\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITKg9Z5qEWRU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "ec69058b-7145-4ca3-be9a-c32b158916af"
      },
      "source": [
        "print(embedding_matrix[word_to_id[b'the']])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.038194   -0.24487001  0.72812003 -0.39961001  0.083172    0.043953\n",
            " -0.39140999  0.3344     -0.57545     0.087459    0.28786999 -0.06731\n",
            "  0.30906001 -0.26383999 -0.13231    -0.20757     0.33395001 -0.33848\n",
            " -0.31742999 -0.48335999  0.1464     -0.37303999  0.34577     0.052041\n",
            "  0.44946    -0.46970999  0.02628    -0.54154998 -0.15518001 -0.14106999\n",
            " -0.039722    0.28277001  0.14393     0.23464    -0.31020999  0.086173\n",
            "  0.20397     0.52623999  0.17163999 -0.082378   -0.71787    -0.41531\n",
            "  0.20334999 -0.12763     0.41367     0.55186999  0.57907999 -0.33476999\n",
            " -0.36559001 -0.54856998 -0.062892    0.26583999  0.30204999  0.99774998\n",
            " -0.80480999 -3.0243001   0.01254    -0.36941999  2.21670008  0.72201002\n",
            " -0.24978     0.92136002  0.034514    0.46744999  1.10790002 -0.19358\n",
            " -0.074575    0.23353    -0.052062   -0.22044     0.057162   -0.15806\n",
            " -0.30798    -0.41624999  0.37972     0.15006    -0.53211999 -0.20550001\n",
            " -1.25259995  0.071624    0.70564997  0.49744001 -0.42063001  0.26148\n",
            " -1.53799999 -0.30223    -0.073438   -0.28312001  0.37103999 -0.25217\n",
            "  0.016215   -0.017099   -0.38984001  0.87423998 -0.72569001 -0.51058\n",
            " -0.52028    -0.1459      0.82779998  0.27061999]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG-XVvTTDZMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_model = keras.Sequential([\n",
        "    keras.layers.Embedding(embed_size,output_dim=100,weights=[embedding_matrix],input_length=100, trainable=False),\n",
        "    keras.layers.GRU(100, activation=\"tanh\",dropout=0.25),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "glove_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVYaUfVeDZMi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "f9d547a0-480a-487a-f9a6-8aea28fb29ef"
      },
      "source": [
        "\n",
        "batch_size = 32\n",
        "history = glove_model.fit(train_set, epochs=13)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/13\n",
            "469/469 [==============================] - 9s 20ms/step - loss: 0.6981 - accuracy: 0.5057\n",
            "Epoch 2/13\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.6423 - accuracy: 0.6138\n",
            "Epoch 3/13\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.5437 - accuracy: 0.7206\n",
            "Epoch 4/13\n",
            "469/469 [==============================] - 5s 12ms/step - loss: 0.5152 - accuracy: 0.7439\n",
            "Epoch 5/13\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.4989 - accuracy: 0.7499\n",
            "Epoch 6/13\n",
            "469/469 [==============================] - 6s 12ms/step - loss: 0.4855 - accuracy: 0.7587\n",
            "Epoch 7/13\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.4741 - accuracy: 0.7652\n",
            "Epoch 8/13\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.4611 - accuracy: 0.7725\n",
            "Epoch 9/13\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.4470 - accuracy: 0.7798\n",
            "Epoch 10/13\n",
            "469/469 [==============================] - 5s 12ms/step - loss: 0.4311 - accuracy: 0.7922\n",
            "Epoch 11/13\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.4208 - accuracy: 0.7993\n",
            "Epoch 12/13\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.4049 - accuracy: 0.8109\n",
            "Epoch 13/13\n",
            "469/469 [==============================] - 5s 11ms/step - loss: 0.3912 - accuracy: 0.8161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wuw88rwKu0z3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "outputId": "35b1e038-49a6-46c5-dae9-06a5ab0057a9"
      },
      "source": [
        "glove_model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 100)          1100000   \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 100)               60600     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,160,701\n",
            "Trainable params: 60,701\n",
            "Non-trainable params: 1,100,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AruxhSSrsEph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set = test_data.batch(32).map(preprocess)\n",
        "test_set = test_set.map(encode_words).prefetch(1)\n",
        "dev_set = dev_data.batch(32).map(preprocess)\n",
        "dev_set = dev_set.map(encode_words).prefetch(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiVyZGPytkYP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5bd7e924-2ee4-43f8-d958-2c960e8b6163"
      },
      "source": [
        "loss,accurracy = glove_model.evaluate(dev_set, steps=10, verbose=True)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 0s 44ms/step - loss: 0.5301 - accuracy: 0.7906\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lu2M9_Dr-AT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73f96649-dd7f-4eeb-c1ae-86916f9073a7"
      },
      "source": [
        "loss,accuracy = glove_model.evaluate(test_set, steps=10,verbose=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 0s 8ms/step - loss: 0.4992 - accuracy: 0.7344\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcuLFwuNsmyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "negative_sentence = b\"I hated this movie totally unbelievable\"\n",
        "positive_sentence = b\"I loved this movie totally unbelievable\"\n",
        "\n",
        "pos_arr = []\n",
        "neg_arr = []\n",
        "\n",
        "count = 0\n",
        "for i in positive_sentence.split():\n",
        "  count += 1\n",
        "  pos_arr.append(word_to_id.get(i) or vocab_size )\n",
        "for i in range(100 - count):\n",
        "  pos_arr.append(word_to_id.get(b\"<pad>\"))\n",
        "count = 0\n",
        "for i in negative_sentence.split():\n",
        "  count += 1\n",
        "  neg_arr.append(word_to_id.get(i) or vocab_size )\n",
        "for i in range(100 - count):\n",
        "  neg_arr.append(word_to_id.get(b\"<pad>\"))\n",
        "\n",
        "pos_arr = np.array(pos_arr)\n",
        "pos_arr = pos_arr.reshape(100,1).T\n",
        "\n",
        "neg_arr = np.array(neg_arr)\n",
        "neg_arr = neg_arr.reshape(100,1).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEr4RsQH6xqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"The glove embedding for '{}' is {} \".format('hated', embedding_matrix[word_to_id[b'hated']]))\n",
        "print(\"The glove embedding for '{}' is {} \".format('loved', embedding_matrix[word_to_id[b'loved']]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmmvKeUVCKFv",
        "colab_type": "text"
      },
      "source": [
        "Cosine embedding check and prediction results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4DdQAi5sfEq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "77bb98d5-5c77-4ee9-e3ec-0fbbf2b29903"
      },
      "source": [
        "print(positive_sentence)\n",
        "print(\"Positive results: {}\".format(glove_model.predict(pos_arr)))\n",
        "\n",
        "print(negative_sentence)\n",
        "print(\"Negative results: {}\".format(glove_model.predict(neg_arr)))\n",
        "\n",
        "cos_sim = dot(pos_arr.flatten(),neg_arr.flatten())/(norm(pos_arr.flatten())*norm(neg_arr.flatten()))\n",
        "\n",
        "print(cos_sim)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'I loved this movie totally unbelievable'\n",
            "Positive results: [[0.98445386]]\n",
            "b'I hated this movie totally unbelievable'\n",
            "Negative results: [[0.21827108]]\n",
            "0.80763806264761\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}