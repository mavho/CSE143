{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2_Part3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mavho/CSE143/blob/master/A2_Part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mYpcy4ILYOl",
        "colab_type": "text"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Installs the correct packages.\n",
        "\n",
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z__lfNuUXsps",
        "colab_type": "code",
        "outputId": "d905e659-ed91-4219-97d8-a4ea92cf5c93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    #pip install -q -U tensorflow-addons\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.test.is_gpu_available():\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"nlp\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-d2f2dcc24676>:21: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlVZr_-iL2yI",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj32n_PKEnNr",
        "colab_type": "text"
      },
      "source": [
        "Below is an example of how to do custom preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSbrtR0nDZLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "train_data, dev_data, test_data = tfds.load(\"imdb_reviews\", split=('train[:60%]', 'train[60%:]', 'test'), as_supervised=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NijMHL1zDZLX",
        "colab_type": "code",
        "outputId": "a2ff0d64-52cc-4f75-d700-75f7d5d56500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "source": [
        "for X_batch, y_batch in train_data.batch(2).take(1):\n",
        "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
        "        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
        "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
        "        print()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
            "Label: 0 = Negative\n",
            "\n",
            "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
            "Label: 0 = Negative\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1lzP_KmDZLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(X_batch, y_batch):\n",
        "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
        "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
        "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
        "    X_batch = tf.strings.split(X_batch)\n",
        "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAoo8pM5FNE7",
        "colab_type": "text"
      },
      "source": [
        "Construct the vocaulary, counts occurence of each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAH0e2-7DZLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "vocabulary = Counter()\n",
        "for X_batch, y_batch in train_data.batch(32).map(preprocess):\n",
        "    for review in X_batch:\n",
        "        vocabulary.update(list(review.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybqSIIBIFffo",
        "colab_type": "text"
      },
      "source": [
        "Look at only the first 10000 most common words in the vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy2djDvwDZLz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = 10000\n",
        "truncated_vocabulary = [\n",
        "    word for word, count in vocabulary.most_common()[:vocab_size]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfvGEfzDHdWk",
        "colab_type": "text"
      },
      "source": [
        "# Using Pretrained Embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB342qdWoDAU",
        "colab_type": "text"
      },
      "source": [
        "Get and extract Glove datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RydXKVKvnpj",
        "colab_type": "code",
        "outputId": "07565a61-c0cc-4e57-f4ef-d2dad19703fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "!rm glove.6B.zip\n",
        "!mkdir glove\n",
        "!mv *.txt glove"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-13 23:21:44--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-02-13 23:21:44--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-02-13 23:21:45--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip         69%[============>       ] 567.80M  1.99MB/s    eta 1m 58s "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbEIy13HDFIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
        "for word in b\"This This this\".split():\n",
        "    print(word_to_id.get(word) or vocab_size)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HyR9ENd2iHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = tf.constant(truncated_vocabulary)\n",
        "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
        "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
        "num_oov_buckets = 1000\n",
        "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsjqMEwU15H4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = train_data.batch(32).map(preprocess)\n",
        "for X_batch, y_batch in train_set.batch(2).take(1):\n",
        "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
        "        print(\"Review:\", review[:200], \"...\")\n",
        "        print(\"Label:\", label)\n",
        "        print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6F7EBejCzsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_words(X_batch, y_batch):\n",
        "    return table.lookup(X_batch), y_batch\n",
        "    \n",
        "train_set = train_set.map(encode_words).prefetch(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-gv8iEg-CGmS",
        "colab": {}
      },
      "source": [
        "for X_batch, y_batch in train_set.batch(1).take(1):\n",
        "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
        "        print(\"Review:\", review[:200], \"...\")\n",
        "        print(\"Label:\", label)\n",
        "        print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjkIyMiHm7cf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = dict()\n",
        "f = open('/content/glove/glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icPw5JSAnBM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_size = vocab_size+num_oov_buckets\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((embed_size, 100))\n",
        "for word, index in word_to_id.items():\n",
        "    if index > embed_size - 1:\n",
        "        break\n",
        "    else:\n",
        "        embedding_vector = embeddings_index.get(word.decode('utf8'))\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITKg9Z5qEWRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(embedding_matrix.argmax())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG-XVvTTDZMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(embed_size,output_dim=100,weights=[embedding_matrix],input_length=100, trainable=False),\n",
        "    keras.layers.GRU(100, activation=\"tanh\",dropout=0.25),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVYaUfVeDZMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "batch_size = 32\n",
        "history = model.fit(train_set, epochs=13)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wuw88rwKu0z3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AruxhSSrsEph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set = test_data.batch(32).map(preprocess)\n",
        "test_set = test_set.map(encode_words).prefetch(1)\n",
        "dev_set = dev_data.batch(32).map(preprocess)\n",
        "dev_set = dev_set.map(encode_words).prefetch(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lu2M9_Dr-AT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss,accuracy = model.evaluate(test_set, steps=10,verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiVyZGPytkYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss,accurracy = model.evaluate(dev_set, steps=10, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}