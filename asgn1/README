Using py3.X

How to run: python3 A1.py

Desc:
The program builds the vocab of unigrams, bigrams, and trigrams from the training set.
Calculates the probabilities of the n-grams, then calculates the perplexities of each file
using these probabilities. We then use linear interpolation to smooth out these perplexities/probabilities.

Output Desc:
The program will output the name of the file it's working on along with the n-gram its using, and the perplexity of that file.
After processing all the n-gram options, we use linear interpolation and calculate the perplexity
of the files again.

Tuning Parameters:
To edit parameters you'll have to go into the A1.py file.

If you only wanted to check the smoothing function, you can comment out line 24: cal_perplexity function.
To change the lambdas, change l1,l2,l3 of the smoothing function to whatever values you want.

If you wanted to change the threshold for word count that contribute to <UNK> change the value on line 44.

File desc:
A1.py: main file that runs as described above.
smoothing.py: a module that provides the smoothing function A1.py uses.





Design Doc:
The first step is to generate the training vocab and replace the unks in all of the
files.
    The function initialize takes the training file and calculates the
    word count as well as replace words with unks based on:
        -appears less than 3 times
        -doesnt appear in the built vocab
Probability dictionaries
    Unigram:
        p(x_i) ==> (Number of times it appears in vocab) / (wc of training)
    
perplexity
    wc = wordcount
    Given three dictionaries associated with the n-grams that give us the probability of an ngram

    M = keep track of word count of each file

    Unigram:
        Go through the files, calculate log2(sentence(sum(p(x_i))))
            Sum up all -log probabilities of tokens in the sentence
        sum up all the p(sentences), divide over M
        2 ^ of result is perplexity

    Bigram:
        Basically the same as unigram...
    Trigram:
        Same?

Smoothing
    use probability dictionaries, choose three lambdas. Add them up respectively.